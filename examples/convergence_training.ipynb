{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convergence Training\n",
    "\n",
    "In convergence training, we implemented a method to continiously generate new data until a reference classifier can't get fooled. In our experiments we accomplished to generate 60.000 new data entries for mnist, while maintaining 95 % prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Configuration\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# Import src folder\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "home = Path(os.path.abspath('')).parent\n",
    "sys.path.append(os.path.join(home, \"src\"))\n",
    "\n",
    "# Now import alva\n",
    "from alva import generate_samples_with_iterative_epsilons\n",
    "\n",
    "# Import custom modules\n",
    "from models import LeNet5, CGanGenerator\n",
    "from data import mnist, PerturbatedMnist\n",
    "from utils import set_random_seed, split_tensor_random, unnormalize_tensor\n",
    "from training import training_loop"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "CFG_PATH_TRAIN=r\"config\\training_config.yaml\"\n",
    "CFG_PATH_DATA = r\"config\\mnist_data_config.yaml\"\n",
    "CFG_PATH_GENERATOR_HPARAMS = r\"config\\generator_hparams_config.yaml\"\n",
    "CFG_PATH_CONVERGENCE = r\"config\\convergence_config.yaml\"\n",
    "\n",
    "# Load Config\n",
    "cfg_hyperparams = OmegaConf.load(CFG_PATH_TRAIN)\n",
    "cfg_data = OmegaConf.load(CFG_PATH_DATA)\n",
    "cfg_generator = OmegaConf.load(CFG_PATH_GENERATOR_HPARAMS)\n",
    "cfg_convergence = OmegaConf.load(CFG_PATH_CONVERGENCE)\n",
    "\n",
    "# Set random seed\n",
    "set_random_seed(0)\n",
    "\n",
    "# Get the device for cuda optimization\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Set Hyperparameters - Training\n",
    "BATCH_SIZE = cfg_hyperparams.BATCH_SIZE\n",
    "LEARNING_RATE = cfg_hyperparams.LEARNING_RATE\n",
    "N_EPOCHS = cfg_hyperparams.N_EPOCHS\n",
    "# Set Hyperparameters - Convergence \n",
    "N_EPSILON_EPOCHS = cfg_convergence.N_CONV_EPOCHS\n",
    "N_GENERATED_SAMPLES = cfg_convergence.N_GENERATED_SAMPLES\n",
    "N_TIMEOUT_TRIES = cfg_convergence.N_TIMEOUT\n",
    "CLASSES = cfg_data.CLASSES\n",
    "\n",
    "# Load Data\n",
    "root = os.path.join(os.path.abspath(os.path.curdir),cfg_data.ROOT)\n",
    "per_training_data = PerturbatedMnist(root,'training', transform = mnist.get_standard_transformation())\n",
    "per_test_data = PerturbatedMnist(root, 'test', transform = mnist.get_standard_transformation())\n",
    "\n",
    "#Load the Generator\n",
    "generator = CGanGenerator(cfg_generator.LATENT_DIM, tuple(cfg_generator.OUTPUT_DIM)).to(DEVICE)\n",
    "generator.load_state_dict(torch.load(r\"models\\state_dicts\\conditional_gan_generator.pt\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper method for convergence training  (see [alva example](alva_example.ipynb) for more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(classifier, generator, device, target, n_generated_samples, n_timeout, print_info : bool = True):\n",
    "    \"\"\"\n",
    "    Generates samples \n",
    "    \"\"\"\n",
    "    # Generate samples\n",
    "    (z, y, per_z, per_y) = generate_samples_with_iterative_epsilons(classifier, generator, device, target, n_generated_samples, n_timeout)\n",
    "    \n",
    "    if print_info:\n",
    "        print(f\"Generated {len(z)} adversarial samples with generator\")\n",
    "    \n",
    "    x, per_x = generator(z).detach().cpu(), generator(per_z).detach().cpu()\n",
    "\n",
    "    return x, y, per_x, per_y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Training consists of three steps\n",
    "\n",
    "Convergence Training consists of following steps:\n",
    "\n",
    "1. Train a classifier $C$ on a dataset $D$.\n",
    "2. Use ALVA to generate $n$ samples from each class in $D$.\n",
    "3. Add the generated samples to $D$.\n",
    "4. Repeat steps 1-3 until ALVA can no longer \"fool\" $C$.\n",
    "\n",
    "The steps are also marked in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epsilon_epoch in range(N_EPSILON_EPOCHS):\n",
    "    # Declare experiment specific variables\n",
    "    run_name = f\"{epsilon_epoch:03}\"\n",
    "\n",
    "    # Reload data and create Dataloader\n",
    "    per_training_data.load_data()\n",
    "    per_test_data.load_data()\n",
    "    train_loader = DataLoader(per_training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(per_test_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    # Start run\n",
    "    print(f'\\n--------------------------')\n",
    "    print(f'Executing Experiment: #{run_name}')\n",
    "    print(f'\\nPerturbated images: {round(per_training_data.get_perturbated_percentage(), 3)}%')\n",
    "    print(f'--------------------------\\n')\n",
    "\n",
    "    # Reinitialize model\n",
    "    classifier = LeNet5().to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train the model on the dataset - STEP 1\n",
    "    print(\"\\nTRAINING\\n\")\n",
    "    classifier, optimizer, metrics = training_loop(classifier, criterion, optimizer, train_loader, test_loader, N_EPOCHS, DEVICE)\n",
    "\n",
    "    # Execute the pipeline to generate samples for every class\n",
    "    per_xs = []\n",
    "    targets = []\n",
    "    all_target_figures = []\n",
    "    epsilons  = []\n",
    "\n",
    "    # STEP 2\n",
    "    print(\"\\nGENERATING\\n\")\n",
    "    for target in CLASSES:\n",
    "        print(\"\\nGenerating data with class  \" + str(target) + \"\\n\")\n",
    "\n",
    "        # Generating images and storing figures \n",
    "        x, y, per_x, per_y = generate_samples(classifier, generator, DEVICE, target, N_GENERATED_SAMPLES, BATCH_SIZE, print_info=False)\n",
    "\n",
    "        # Append data\n",
    "        per_xs.append(unnormalize_tensor(per_x))\n",
    "        targets.append(torch.full((1, per_x.shape[0]), target, dtype=int))\n",
    "\n",
    "    # Concatenate date and split\n",
    "    per_xs = torch.cat(per_xs).view(-1, 28,28)\n",
    "    targets = torch.cat(targets, dim=1).view(-1)\n",
    "    # Process data\n",
    "    x_test, y_test, x_train, y_train = split_tensor_random(per_xs, targets)\n",
    "\n",
    "    # Save generated data - STEP 3\n",
    "    train_path = os.path.join(per_training_data.get_perturbated_data_dir() , f'{run_name}_training.pt')\n",
    "    test_path = os.path.join(per_test_data.get_perturbated_data_dir(), f'{run_name}_test.pt')\n",
    "    torch.save((x_train, y_train), train_path)\n",
    "    torch.save((x_test, y_test), test_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
